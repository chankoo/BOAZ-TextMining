{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chankoo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "##w word2vec을 쓰기 위해서 gensim을 다운받아야 한다\n",
    "## terminal에 easy_install -U gensim 혹은 pip install --upgrade gensim을 친다\n",
    "\n",
    "#설치오류시 참고\n",
    "##https://blog.naver.com/sans223/221274010123\n",
    "##https://blog.naver.com/ddonae_/221190968528\n",
    "\n",
    "from glob import glob\n",
    "from codecs import open as codecs_open\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from os import path, mkdir\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.manifold import TSNE\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books\\\\HPBook1.txt',\n",
       " 'books\\\\HPBook2.txt',\n",
       " 'books\\\\HPBook3.txt',\n",
       " 'books\\\\HPBook4.txt',\n",
       " 'books\\\\HPBook5.txt',\n",
       " 'books\\\\HPBook6.txt',\n",
       " 'books\\\\HPBook7.txt',\n",
       " 'books\\\\HPBook8.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 경로를 지정하고 파일을 읽어온다 바탕화면에 다운로드 할 경우 Users와 user를 본인 컴에 맞게 지정할 것\n",
    "\n",
    "def read_books(location):\n",
    "    if path.exists(location):\n",
    "        return sorted(glob(path.join(location, \"*.txt\")))\n",
    "    else:\n",
    "        raise NotADirectoryError(location)\n",
    "books = read_books(r'''books''')\n",
    "read_books(r'''books''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading books\\HPBook1.txt\n",
      "Corpus is now 449988 characters long\n",
      "Reading books\\HPBook2.txt\n",
      "Corpus is now 949340 characters long\n",
      "Reading books\\HPBook3.txt\n",
      "Corpus is now 1575163 characters long\n",
      "Reading books\\HPBook4.txt\n",
      "Corpus is now 2688627 characters long\n",
      "Reading books\\HPBook5.txt\n",
      "Corpus is now 4204918 characters long\n",
      "Reading books\\HPBook6.txt\n",
      "Corpus is now 5209621 characters long\n",
      "Reading books\\HPBook7.txt\n",
      "Corpus is now 6404966 characters long\n",
      "Reading books\\HPBook8.txt\n",
      "Corpus is now 6681889 characters long\n"
     ]
    }
   ],
   "source": [
    "## corpus 생성\n",
    "def create_corpus(books):\n",
    "    raw_corpus = u''\n",
    "    for book in books:\n",
    "        print(\"Reading {0}\".format(book))\n",
    "        with codecs_open(book, 'r', 'utf-8') as book_file:\n",
    "            raw_corpus += book_file.read()\n",
    "        print(\"Corpus is now {0} characters long\".format(len(raw_corpus)))\n",
    "    return raw_corpus\n",
    "raw_corpus2 = create_corpus(books)\n",
    "\n",
    "##뒤에 불용어 처리할때 (stop_word) 소문자 감안 하여 미리 소문자로 바꿔준다 (현중 : 나중에 바꾸는 것을 추천, 대문자 필요할 때 있으니까)\n",
    "raw_corpus = raw_corpus2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장 단위의 token을 생성 \n",
    "def tokenize_corpus(raw_corpus):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    return tokenizer.tokenize(raw_corpus)\n",
    "\n",
    "tokenize_corpus(raw_corpus)\n",
    "token = tokenize_corpus(raw_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'one', 'r', 'n', 'r', 'n', 'r', 'nthe', 'boy', 'lived']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 단위의 token을 word로 쪼갠다\n",
    "def sentence_to_words_list(token):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", str(token)).split()\n",
    "    return [word for word in words if word not in stop_words]\n",
    "tokens = sentence_to_words_list(token)\n",
    "sentence_to_words_list(token)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 598,608 tokens\n"
     ]
    }
   ],
   "source": [
    "## 세글자 이상의 단어만 수집하기로 한다(r, n 이런 것 때문에!) 시간이 걸린다 \n",
    "def tokens_to_words(tokens):\n",
    "    words = [sentence_to_words_list(token) for token in tokens if len(token) > 2]\n",
    "    print(\"The corpus contains {0:,} tokens\".format(sum([len(word) for word in words])))\n",
    "    return words\n",
    "\n",
    "words = tokens_to_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## workers = multiprocessing.cpu_count(), \n",
    "##min_count = 50 등등장횟수 50이하인 단어는 제외, \n",
    "##size=100 100개의 차원으로 embedding\n",
    "## sg=0 이면 CBOW sg=1이면 skip.gram  (sg=skip.gram)\n",
    "\n",
    "def we_build_vocab(words, num_features, min_word_count, num_workers, context_size):\n",
    "    word2vec = Word2Vec(sg=1, \n",
    "                        size=num_features,   \n",
    "                        min_count=min_word_count, \n",
    "                        workers=num_workers,\n",
    "                        window=context_size)\n",
    "    print(\"Building Vocabulary\")\n",
    "    word2vec.build_vocab(words)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocabulary\n"
     ]
    }
   ],
   "source": [
    "num_core = cpu_count() #cpu코어수\n",
    "model = we_build_vocab(words,100,10,num_core,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5058333, 5986080)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(words,total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델을 저장하고 불러와서 다시 training시킬 수 있다\n",
    "model.save(r'''model_HP.w2v''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(r'''model_HP.w2v''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00137161,  0.00440795, -0.00256591, -0.00267156, -0.00301022,\n",
       "        0.00051129, -0.00040498,  0.00045905, -0.00077698, -0.00403777,\n",
       "        0.0039246 ,  0.00339091, -0.00205718,  0.00413822, -0.00055404,\n",
       "       -0.00316601, -0.00080426, -0.00465249, -0.00059959, -0.00244771,\n",
       "        0.00201681, -0.00018985,  0.00393345, -0.00410878, -0.00350473,\n",
       "        0.00440481, -0.00124123,  0.00029857,  0.00168274,  0.00352092,\n",
       "       -0.0011828 , -0.00208766, -0.00226902, -0.00141631,  0.0041537 ,\n",
       "        0.00312662,  0.00434745, -0.00083323, -0.004753  ,  0.00363353,\n",
       "        0.0001323 ,  0.00100896,  0.00196907,  0.00467548, -0.00274274,\n",
       "       -0.00090765, -0.00368002,  0.00165301,  0.00070999, -0.0025635 ,\n",
       "       -0.00074786, -0.00459201,  0.00388561,  0.00145363,  0.0030401 ,\n",
       "        0.00348675, -0.00072612,  0.00465468,  0.00200209,  0.00018606,\n",
       "       -0.00179823, -0.0014523 , -0.00334594,  0.00361283,  0.00280403,\n",
       "       -0.00313029,  0.00335987,  0.00129711,  0.00495531, -0.00277181,\n",
       "       -0.00497728,  0.00142775, -0.00365489, -0.00404577, -0.00085524,\n",
       "        0.00061245,  0.00459554,  0.00130589, -0.0049954 ,  0.00065325,\n",
       "        0.00115506,  0.00220364, -0.00438225, -0.00101133, -0.0025299 ,\n",
       "       -0.00466051,  0.00352906,  0.00292751, -0.00277198, -0.00060291,\n",
       "       -0.00172563,  0.00089234,  0.00334896, -0.00381873,  0.00194836,\n",
       "        0.00039992, -0.0044693 ,  0.00234398, -0.00377758, -0.0048233 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'볼드모트' 를 임베딩한 벡터\n",
    "model.wv['voldemort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word_vector와 내장함수로 여러가지를 구해볼 수 있다\n",
    "def word_correlation(word_vector, a, b, c):\n",
    "    return word_vector.most_similar_cosmul(positive=[a, c], negative=[b])[0][0]\n",
    "\n",
    "\n",
    "def word_find_most_similar(word_vector, word):\n",
    "    return word_vector.most_similar(word)[0][0]\n",
    "\n",
    "\n",
    "def word_odd_one(word_vector, phrase):\n",
    "    return word_vector.doesnt_match(phrase.split())\n",
    "\n",
    "def similarity(word_vector,a,b):\n",
    "    return word_vector.similarity(a,b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering\n",
      "north\n",
      "stranger\n",
      "-0.006215989860382375\n"
     ]
    }
   ],
   "source": [
    "## 현재 학습이 제대로 안됨\n",
    "print(word_correlation(model.wv, 'harry', 'voldemort', 'ron'))\n",
    "print(word_find_most_similar(model.wv, 'ron'))\n",
    "print(word_odd_one(model.wv,'He had been hugged by a complete stranger'))\n",
    "print(similarity(model.wv,'harry','ron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
